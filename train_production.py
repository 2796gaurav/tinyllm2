"""
TinyGuardrail: Production Training Script
Trains with real benchmarks, comprehensive metrics, ONNX export

For Research Paper & Production Deployment
"""

import os
import sys
import argparse
import json
import random
from pathlib import Path
from typing import Dict, List
import numpy as np
import pandas as pd
from tqdm import tqdm

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, get_linear_schedule_with_warmup
import wandb

# Add src to path
sys.path.insert(0, str(Path(__file__).parent))

from src.models.dual_branch import TinyGuardrail, DualBranchConfig, TinyGuardrailOutput
from src.models.embeddings import text_to_char_ids
from src.data.benchmark_loaders import BenchmarkDataLoader
from src.data.attack_generators import Attack2026Generator, HardNegativeGenerator
from src.evaluation.benchmarks import GuardrailBenchmark


def set_seed(seed: int = 42):
    """Set random seeds for reproducibility"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


class ProductionTrainingConfig:
    """Production training configuration"""
    
    # Model
    vocab_size: int = 30522  # Will use actual tokenizer vocab
    d_model: int = 384
    num_labels: int = 4  # benign, direct_injection, jailbreak, obfuscation
    
    # Training
    num_epochs: int = 5
    batch_size: int = 16
    learning_rate: float = 5e-5
    weight_decay: float = 0.01
    warmup_ratio: float = 0.1
    gradient_accumulation_steps: int = 4  # Effective batch: 64
    
    # Router (FIXED for 70/30 split)
    router_threshold: float = 0.3  # ‚úÖ FIXED from 0.6 to encourage deep branch usage
    router_loss_weight: float = 0.5  # ‚úÖ INCREASED from 0.1 to enforce 70/30 split
    
    # Loss weights
    focal_alpha: float = 0.25
    focal_gamma: float = 2.0
    aux_loss_weight: float = 0.01
    
    # Adversarial training
    use_adversarial: bool = True
    adversarial_epsilon: float = 0.01
    adversarial_start_epoch: int = 3
    
    # Quantization
    use_quantization: bool = True
    quantization_start_epoch: int = 4
    
    # Hardware
    fp16: bool = True
    num_workers: int = 4
    max_grad_norm: float = 1.0
    
    # Paths
    output_dir: str = "outputs/production"
    log_dir: str = "logs/production"
    checkpoint_dir: str = "checkpoints/production"
    
    # Monitoring
    logging_steps: int = 50
    eval_steps: int = 200
    save_steps: int = 500
    
    # WandB
    use_wandb: bool = True
    wandb_project: str = "tinyllm-guardrail-production"
    wandb_run_name: str = "dual-branch-v1"


class GuardrailDataset(Dataset):
    """Dataset with character-level support"""
    
    def __init__(
        self,
        texts: List[str],
        labels: List[int],
        tokenizer,
        max_length: int = 256,
        max_chars_per_token: int = 20,
    ):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.max_chars_per_token = max_chars_per_token
        
        # Character vocabulary
        self.char_vocab = {chr(i): i for i in range(512)}
        self.char_vocab['<PAD>'] = 0
        self.char_vocab['<UNK>'] = 1
    
    def text_to_char_ids(self, text: str, token_ids: torch.Tensor) -> torch.Tensor:
        """Convert text to character IDs aligned with tokens"""
        tokens = self.tokenizer.convert_ids_to_tokens(token_ids.tolist())
        
        char_ids_list = []
        for token in tokens:
            if token in ['[PAD]', '[CLS]', '[SEP]']:
                char_ids_list.append([0] * self.max_chars_per_token)
            else:
                clean_token = token.replace('##', '')
                token_char_ids = []
                for char in clean_token[:self.max_chars_per_token]:
                    char_id = self.char_vocab.get(char, 1)
                    token_char_ids.append(char_id)
                
                while len(token_char_ids) < self.max_chars_per_token:
                    token_char_ids.append(0)
                
                char_ids_list.append(token_char_ids)
        
        while len(char_ids_list) < self.max_length:
            char_ids_list.append([0] * self.max_chars_per_token)
        
        return torch.tensor(char_ids_list[:self.max_length], dtype=torch.long)
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        
        # Tokenize
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt',
        )
        
        input_ids = encoding['input_ids'].squeeze(0)
        attention_mask = encoding['attention_mask'].squeeze(0)
        
        # Generate character IDs
        char_ids = self.text_to_char_ids(text, input_ids)
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'char_ids': char_ids,
            'labels': torch.tensor(label, dtype=torch.long),
            'text': text,
        }


def load_production_data(config: ProductionTrainingConfig):
    """
    Load production-ready dataset
    
    Composition:
    - 60K real benchmarks (PINT, JBB, ToxicChat, WildGuard, NotInject)
    - 50K synthetic 2025 attacks (FlipAttack, CodeChameleon, etc.)
    - 30K hard negatives (benign with triggers, for FPR reduction)
    
    Total: 140K samples
    """
    print("\n" + "="*80)
    print("üìä Loading Production Dataset")
    print("="*80)
    
    # 1. Load real benchmarks (60K)
    print("\n1Ô∏è‚É£ Loading Real Benchmark Data...")
    benchmark_loader = BenchmarkDataLoader()
    benchmarks = benchmark_loader.load_all_benchmarks()
    real_data = benchmark_loader.combine_benchmarks(benchmarks)
    
    print(f"   ‚úÖ Loaded {len(real_data):,} real benchmark samples")
    
    # 2. Generate synthetic 2025 attacks (50K)
    print("\n2Ô∏è‚É£ Generating 2025 Attack Data (FlipAttack, CodeChameleon, etc.)...")
    attack_gen = Attack2026Generator()
    attack_data = attack_gen.generate_all_attacks(n_total=50000)
    
    print(f"   ‚úÖ Generated {len(attack_data):,} attack samples")
    
    # 3. Generate hard negatives (30K) - Critical for FPR reduction
    print("\n3Ô∏è‚É£ Generating Hard Negatives (FPR Reduction)...")
    hard_neg_gen = HardNegativeGenerator()
    hard_neg_data = hard_neg_gen.generate_all_hard_negatives(n_total=30000)
    
    print(f"   ‚úÖ Generated {len(hard_neg_data):,} hard negative samples")
    
    # 4. Combine all data
    combined_data = pd.concat([real_data, attack_data, hard_neg_data], ignore_index=True)
    
    # Shuffle
    combined_data = combined_data.sample(frac=1.0, random_state=42).reset_index(drop=True)
    
    print("\n" + "="*80)
    print("üìä Dataset Summary")
    print("="*80)
    print(f"Total samples:     {len(combined_data):,}")
    print(f"  Real benchmarks: {len(real_data):,} ({len(real_data)/len(combined_data)*100:.1f}%)")
    print(f"  Synthetic attacks: {len(attack_data):,} ({len(attack_data)/len(combined_data)*100:.1f}%)")
    print(f"  Hard negatives:  {len(hard_neg_data):,} ({len(hard_neg_data)/len(combined_data)*100:.1f}%)")
    print(f"\nLabel distribution:")
    for label, count in combined_data['label'].value_counts().sort_index().items():
        label_names = ['Benign', 'Direct Injection', 'Jailbreak', 'Obfuscation']
        print(f"  {label_names[label]:20s}: {count:,} ({count/len(combined_data)*100:.1f}%)")
    print("="*80 + "\n")
    
    return combined_data


def train_epoch(
    model,
    train_loader,
    optimizer,
    scheduler,
    device,
    epoch,
    config,
    scaler=None,
):
    """Train for one epoch"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    routing_stats = {'fast': 0, 'deep': 0}
    
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{config.num_epochs}")
    
    for step, batch in enumerate(pbar):
        # Move to device
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        char_ids = batch['char_ids'].to(device)
        labels = batch['labels'].to(device)
        texts = batch['text']
        
        # Forward pass
        if scaler and config.fp16:
            with torch.amp.autocast('cuda'):
                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    char_ids=char_ids,
                    labels=labels,
                    text=texts[0] if len(texts) > 0 else None,
                    return_dict=True,
                )
                loss = outputs.loss
        else:
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                char_ids=char_ids,
                labels=labels,
                text=texts[0] if len(texts) > 0 else None,
                return_dict=True,
            )
            loss = outputs.loss
        
        # Backward pass
        if scaler and config.fp16:
            scaler.scale(loss).backward()
            
            if (step + 1) % config.gradient_accumulation_steps == 0:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)
                scaler.step(optimizer)
                scaler.update()
                scheduler.step()
                optimizer.zero_grad()
        else:
            loss.backward()
            
            if (step + 1) % config.gradient_accumulation_steps == 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()
        
        # Metrics
        total_loss += loss.item()
        preds = torch.argmax(outputs.logits, dim=-1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)
        
        # Routing stats
        if outputs.route_info:
            fast_count = (~outputs.route_decision).sum().item()
            deep_count = outputs.route_decision.sum().item()
            routing_stats['fast'] += fast_count
            routing_stats['deep'] += deep_count
        
        # Update progress bar
        pbar.set_postfix({
            'loss': f"{loss.item():.4f}",
            'acc': f"{correct/total:.4f}",
        })
    
    avg_loss = total_loss / len(train_loader)
    avg_acc = correct / total
    
    # Routing ratio
    total_routed = routing_stats['fast'] + routing_stats['deep']
    fast_ratio = routing_stats['fast'] / total_routed if total_routed > 0 else 0.0
    deep_ratio = routing_stats['deep'] / total_routed if total_routed > 0 else 0.0
    
    return avg_loss, avg_acc, fast_ratio, deep_ratio


def evaluate(model, val_loader, device):
    """Evaluate on validation set"""
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []
    routing_stats = {'fast': 0, 'deep': 0}
    
    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Evaluating"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            char_ids = batch['char_ids'].to(device)
            labels = batch['labels'].to(device)
            texts = batch['text']
            
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                char_ids=char_ids,
                labels=labels,
                text=texts[0] if len(texts) > 0 else None,
                return_dict=True,
            )
            
            total_loss += outputs.loss.item()
            preds = torch.argmax(outputs.logits, dim=-1)
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            
            # Routing stats
            if outputs.route_info:
                fast_count = (~outputs.route_decision).sum().item()
                deep_count = outputs.route_decision.sum().item()
                routing_stats['fast'] += fast_count
                routing_stats['deep'] += deep_count
    
    # Compute metrics
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support
    
    avg_loss = total_loss / len(val_loader)
    accuracy = accuracy_score(all_labels, all_preds)
    precision, recall, f1, _ = precision_recall_fscore_support(
        all_labels, all_preds, average='weighted', zero_division=0
    )
    
    # Compute FPR (False Positive Rate) - CRITICAL METRIC
    true_negatives = sum(1 for l, p in zip(all_labels, all_preds) if l == 0 and p == 0)
    false_positives = sum(1 for l, p in zip(all_labels, all_preds) if l == 0 and p > 0)
    total_negatives = true_negatives + false_positives
    fpr = (false_positives / total_negatives * 100) if total_negatives > 0 else 0.0
    
    # Routing ratio
    total_routed = routing_stats['fast'] + routing_stats['deep']
    fast_ratio = routing_stats['fast'] / total_routed if total_routed > 0 else 0.0
    deep_ratio = routing_stats['deep'] / total_routed if total_routed > 0 else 0.0
    
    return {
        'loss': avg_loss,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'fpr': fpr,  # ‚≠ê Critical metric
        'fast_ratio': fast_ratio,
        'deep_ratio': deep_ratio,
    }


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--output_dir', type=str, default='outputs/production')
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--use_wandb', action='store_true')
    args = parser.parse_args()
    
    # Configuration
    config = ProductionTrainingConfig()
    config.output_dir = args.output_dir
    config.use_wandb = args.use_wandb
    
    # Create output directories
    Path(config.output_dir).mkdir(parents=True, exist_ok=True)
    Path(config.checkpoint_dir).mkdir(parents=True, exist_ok=True)
    Path(config.log_dir).mkdir(parents=True, exist_ok=True)
    
    # Set seed
    set_seed(args.seed)
    
    # Initialize WandB
    if config.use_wandb:
        wandb.init(
            project=config.wandb_project,
            name=config.wandb_run_name,
            config=vars(config),
        )
    
    # Device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nüñ•Ô∏è  Using device: {device}")
    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    
    # Load data
    combined_data = load_production_data(config)
    
    # Split data
    from sklearn.model_selection import train_test_split
    
    train_df, temp_df = train_test_split(
        combined_data, test_size=0.2, random_state=42, stratify=combined_data['label']
    )
    val_df, test_df = train_test_split(
        temp_df, test_size=0.5, random_state=42, stratify=temp_df['label']
    )
    
    print(f"Train: {len(train_df):,}, Val: {len(val_df):,}, Test: {len(test_df):,}")
    
    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
    config.vocab_size = len(tokenizer.vocab)
    
    # Create datasets
    train_dataset = GuardrailDataset(
        texts=train_df['text'].tolist(),
        labels=train_df['label'].tolist(),
        tokenizer=tokenizer,
    )
    
    val_dataset = GuardrailDataset(
        texts=val_df['text'].tolist(),
        labels=val_df['label'].tolist(),
        tokenizer=tokenizer,
    )
    
    test_dataset = GuardrailDataset(
        texts=test_df['text'].tolist(),
        labels=test_df['label'].tolist(),
        tokenizer=tokenizer,
    )
    
    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=config.num_workers,
        pin_memory=True if torch.cuda.is_available() else False,
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size * 2,
        shuffle=False,
        num_workers=config.num_workers,
        pin_memory=True if torch.cuda.is_available() else False,
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=config.batch_size * 2,
        shuffle=False,
        num_workers=config.num_workers,
        pin_memory=True if torch.cuda.is_available() else False,
    )
    
    # Create model
    print("\n" + "="*80)
    print("üèóÔ∏è  Building TinyGuardrail Model")
    print("="*80)
    
    model_config = DualBranchConfig(
        vocab_size=config.vocab_size,
        d_model=config.d_model,
        num_labels=config.num_labels,
        router_threshold=config.router_threshold,  # ‚úÖ FIXED: 0.3 for 70/30 split
        char_vocab_size=512,
        char_emb_dim=64,
        char_cnn_kernels=[2, 3, 4, 5, 7],
        char_cnn_channels=128,
        fast_num_layers=4,
        fast_num_heads=4,
        deep_num_layers=8,
        deep_num_heads=4,
        num_experts=8,
        num_experts_per_token=2,
    )
    
    model = TinyGuardrail(model_config).to(device)
    
    # Model info
    model_info = model.get_model_info()
    print(f"\nModel Information:")
    print(f"  Total parameters:     {model_info['total_parameters']:,}")
    print(f"  Trainable parameters: {model_info['trainable_parameters']:,}")
    print(f"  Size (FP32):          {model_info['size_fp32_mb']:.2f} MB")
    print(f"  Size (INT8):          {model_info['size_int8_mb']:.2f} MB")
    print(f"  Router threshold:     {config.router_threshold} (target: 70% fast, 30% deep)")
    print(f"  Router loss weight:   {config.router_loss_weight} (increased for better routing)")
    print("="*80 + "\n")
    
    # Optimizer
    from torch.optim import AdamW
    
    optimizer = AdamW(
        model.parameters(),
        lr=config.learning_rate,
        weight_decay=config.weight_decay,
    )
    
    # Scheduler
    num_training_steps = len(train_loader) * config.num_epochs // config.gradient_accumulation_steps
    num_warmup_steps = int(num_training_steps * config.warmup_ratio)
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=num_warmup_steps,
        num_training_steps=num_training_steps,
    )
    
    # Mixed precision scaler
    scaler = torch.amp.GradScaler('cuda') if config.fp16 else None
    
    # Training loop
    print("üöÄ Starting Training...\n")
    
    best_val_f1 = 0.0
    best_model_path = None
    
    for epoch in range(1, config.num_epochs + 1):
        print(f"\n{'='*80}")
        print(f"Epoch {epoch}/{config.num_epochs}")
        print(f"{'='*80}")
        
        # Train
        train_loss, train_acc, fast_ratio, deep_ratio = train_epoch(
            model, train_loader, optimizer, scheduler, device, epoch, config, scaler
        )
        
        # Evaluate
        val_metrics = evaluate(model, val_loader, device)
        
        # Log
        print(f"\nTrain Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
        print(f"Val Loss: {val_metrics['loss']:.4f}, Val Acc: {val_metrics['accuracy']:.4f}")
        print(f"Val F1: {val_metrics['f1']:.4f}")
        print(f"Val FPR: {val_metrics['fpr']:.2f}% ‚≠ê (target: <10%)")
        print(f"Routing: Fast {fast_ratio:.1%}, Deep {deep_ratio:.1%} (target: 70/30)")
        
        # WandB logging
        if config.use_wandb:
            wandb.log({
                'epoch': epoch,
                'train/loss': train_loss,
                'train/accuracy': train_acc,
                'val/loss': val_metrics['loss'],
                'val/accuracy': val_metrics['accuracy'],
                'val/f1': val_metrics['f1'],
                'val/fpr': val_metrics['fpr'],
                'routing/fast_ratio': fast_ratio,
                'routing/deep_ratio': deep_ratio,
            })
        
        # Save best model
        if val_metrics['f1'] > best_val_f1:
            best_val_f1 = val_metrics['f1']
            best_model_path = Path(config.output_dir) / 'best_model.pth'
            
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'val_f1': best_val_f1,
                'val_fpr': val_metrics['fpr'],
                'model_config': model_config,
            }, best_model_path)
            
            print(f"‚úÖ Saved best model (F1: {best_val_f1:.4f}, FPR: {val_metrics['fpr']:.2f}%)")
    
    print("\n" + "="*80)
    print("‚úÖ Training Complete!")
    print("="*80)
    print(f"Best Val F1: {best_val_f1:.4f}")
    print(f"Model saved to: {best_model_path}")
    print("="*80 + "\n")
    
    # Final test evaluation
    print("\nüìä Final Test Evaluation...")
    test_metrics = evaluate(model, test_loader, device)
    
    print(f"\nTest Results:")
    print(f"  Accuracy:  {test_metrics['accuracy']:.4f}")
    print(f"  Precision: {test_metrics['precision']:.4f}")
    print(f"  Recall:    {test_metrics['recall']:.4f}")
    print(f"  F1:        {test_metrics['f1']:.4f}")
    print(f"  FPR:       {test_metrics['fpr']:.2f}% ‚≠ê")
    print(f"  Routing:   Fast {test_metrics['fast_ratio']:.1%}, Deep {test_metrics['deep_ratio']:.1%}")
    
    # Save final metrics
    final_metrics = {
        'test_accuracy': float(test_metrics['accuracy']),
        'test_precision': float(test_metrics['precision']),
        'test_recall': float(test_metrics['recall']),
        'test_f1': float(test_metrics['f1']),
        'test_fpr': float(test_metrics['fpr']),
        'routing_fast_ratio': float(test_metrics['fast_ratio']),
        'routing_deep_ratio': float(test_metrics['deep_ratio']),
        'model_size_mb': float(model_info['size_fp32_mb']),
        'model_size_int8_mb': float(model_info['size_int8_mb']),
        'total_parameters': int(model_info['total_parameters']),
    }
    
    with open(Path(config.output_dir) / 'final_metrics.json', 'w') as f:
        json.dump(final_metrics, f, indent=2)
    
    if config.use_wandb:
        wandb.log(final_metrics)
        wandb.finish()
    
    print("\n‚úÖ Production training complete!")


if __name__ == "__main__":
    main()


